---
layout: post
title: '学习一下golang   练习70  web crawler (网络爬虫)'
date: 2013-03-29 22:01
comments: true
tags: ['golang','crawler','爬虫']
---

之前找了有下web crawler的练习答案. 貌似中文的不多。另外golang.org自从在外面之后， [ 帐前卒
](http://blog.csdn.net/cctt_1) 每次上都需要到那里都要花些功夫。国内的也有一个移植的（ [ 猛击这里
](http://www.aqee.net/go/a-tour-of-go/#1) ）。

最近支付宝的页面也被爬虫爆出来了。不过这应该很久之前的事情了。因为看到了google的搜索，还有2012年8月份的。估计上支付宝那个shenghuo.ali
pay.com这个domain自从上线就没有加robots.txt. 有兴趣的可以在google上使用 site:shenghuo.alipay.com
or 查看 shenghuo.alipay.com/robots.txt.

言归正传, golang的问题是这样的：

    package main
    
    import (
    	"fmt"
    )
    
    type Fetcher interface {
            // Fetch 返回 URL 的 body 内容，并且将在这个页面上找到的 URL 放到一个 slice 中。
    	Fetch(url string) (body string, urls []string, err error)
    }
    
    // Crawl 使用 fetcher 从某个 URL 开始递归的爬取页面，直到达到最大深度。
    func Crawl(url string, depth int, fetcher Fetcher) {
            // TODO: 并行的抓取 URL。
            // TODO: 不重复抓取页面。
            // 下面并没有实现上面两种情况：
    	if depth <= 0 {
    		return
    	}
    	body, urls, err := fetcher.Fetch(url)
    	if err != nil {
    		fmt.Println(err)
    		return
    	}
    	fmt.Printf("found: %s %q\n", url, body)
    	for _, u := range urls {
    		Crawl(u, depth-1, fetcher)
    	}
    	return
    }
    
    func main() {
    	Crawl("http://golang.org/", 4, fetcher)
    }
    // fakeFetcher 是返回若干结果的 Fetcher。
    type fakeFetcher map[string]*fakeResult
    
    type fakeResult struct {
    	body string
    	urls     []string
    }
    
    func (f *fakeFetcher) Fetch(url string) (string, []string, error) {
    	if res, ok := (*f)[url]; ok {
    		return res.body, res.urls, nil
    	}
    	return "", nil, fmt.Errorf("not found: %s", url)
    }
    
    // fetcher 是填充后的 fakeFetcher。
    var fetcher = &fakeFetcher{
    	"http://golang.org/": &fakeResult{
    		"The Go Programming Language",
    		[]string{
    			"http://golang.org/pkg/",
    			"http://golang.org/cmd/",
    		},
    	},
    	"http://golang.org/pkg/": &fakeResult{
    		"Packages",
    		[]string{
    			"http://golang.org/",
    			"http://golang.org/cmd/",
    			"http://golang.org/pkg/fmt/",
    			"http://golang.org/pkg/os/",
    		},
    	},
    	"http://golang.org/pkg/fmt/": &fakeResult{
    		"Package fmt",
    		[]string{
    			"http://golang.org/",
    			"http://golang.org/pkg/",
    		},
    	},
    	"http://golang.org/pkg/os/": &fakeResult{
    		"Package os",
    		[]string{
    			"http://golang.org/",
    			"http://golang.org/pkg/",
    		},
    	},
    }
然后下面是 [ 小卒 ](http://chillyc.info) 的解答。最主要的是改写原来的函数。同步map,
然后父节点应在全部子节点退出后再退出。用channel当banner派上用场。当然golang还提供了锁和其他的同步机制。不过 [ 帐前 卒
](http://blog.csdn.net/cctt_1) 还是先用channel吧。另外最好先看看golang的memory model.

下面 [ 帐 前卒 ](http://chillyc.info) 的代码：

    package main
    
    import (
      "fmt"
    )
    
    type Fetcher interface {
            // Fetch 返回 URL 的 body 内容，并且将在这个页面上找到的 URL 放到一个 slice 中。
      Fetch(url string) (body string, urls []string, err error)
    }
    var lockx = make(chan int,1)
    // 同步通信使用
    func LockFun(f func()) {
      lockx<-1
      f()
      <-lockx
    }
    var visited map[string]bool = make(map[string]bool)
    // Crawl 使用 fetcher 从某个 URL 开始递归的爬取页面，直到达到最大深度。
    func Crawl(url string, depth int, fetcher Fetcher, banner chan int) {
     
      if depth <= 0 || visited[url] {
        banner<-1
        return
      }
      body, urls, err := fetcher.Fetch(url)
      LockFun(func(){
        visited[url]=true
      })
      fmt.Printf("found: %s %q\n", url, body)
      if err != nil {
        fmt.Println(err)
        banner<-1
        return
      }
      subBanner := make(chan int, len(urls))
      for _, u := range urls {
         // 并行吧～～ 
          go Crawl(u, depth-1, fetcher, subBanner);
      }
      for i:=0; i < len(urls); i++ {
        // subBanner用来防止退出
        <-subBanner
      }
      // banner用于让父节点退出
      banner<-1
      return
    }
    
    func main() {
      mainBanner := make(chan int,1)
      Crawl("http://golang.org/", 4, fetcher, mainBanner)
      <-mainBanner
    }
    // fakeFetcher 是返回若干结果的 Fetcher。
    type fakeFetcher map[string]*fakeResult
    
    type fakeResult struct {
      body string
      urls     []string
    }
    
    func (f *fakeFetcher) Fetch(url string) (string, []string, error) {
      if res, ok := (*f)[url]; ok {
        return res.body, res.urls, nil
      }
      return "", nil, fmt.Errorf("not found: %s", url)
    }
    
    // fetcher 是填充后的 fakeFetcher。
    var fetcher = &fakeFetcher{
      "http://golang.org/": &fakeResult{
        "The Go Programming Language",
        []string{
          "http://golang.org/pkg/",
          "http://golang.org/cmd/",
        },
      },
      "http://golang.org/pkg/": &fakeResult{
        "Packages",
        []string{
          "http://golang.org/",
          "http://golang.org/cmd/",
          "http://golang.org/pkg/fmt/",
          "http://golang.org/pkg/os/",
        },
      },
      "http://golang.org/pkg/fmt/": &fakeResult{
        "Package fmt",
        []string{
          "http://golang.org/",
          "http://golang.org/pkg/",
        },
      },
      "http://golang.org/pkg/os/": &fakeResult{
        "Package os",
        []string{
          "http://golang.org/",
          "http://golang.org/pkg/",
        },
      },
    }
觉得没有找到类似python的遍历list但是不需要获取值的方法例如 for _, _ := range urls {}
这样就是错误的。另外golang也不能加入不需要的包，变量。帐 前 卒觉得这点...太洁癖了。

暂时就这样吧。

